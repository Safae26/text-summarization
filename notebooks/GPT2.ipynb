{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6086efb318fd40ec8268bfc76870cb8b",
      "a3cfa1074ec04645b04589877f033c06",
      "1e9364be982040ec99adc9aeea618b76",
      "465dc95b14ef468fa5d1a725237918ef",
      "2bf1b140b3f24fe58a32f44afa1be98b",
      "ab74299e791f4bf7b334f968476f1ea7",
      "1e17b23ee018422b85f77ec2940e4eae",
      "47d2431fafa24761b82fd474c77cee50",
      "1c26102c34ed4194a9b8892d0c678ea2",
      "ccf858fa5811408c8cef8630c07f4962",
      "1f3f78c41b894d19bdf26720885ab926",
      "1399b6599673411aaaddfbd0fe09ef10",
      "f76f95597af840b79f0c629620207883",
      "0094a884c0ea4d87b12765133b2612b4",
      "9704440d598346a99d9f5ff9b315d1a8",
      "e5414688df1541a7b71558898fbc36c4",
      "abc91cb5433849f98097c1e5c0837c92",
      "f79059326b5544e0825abd353ae58cb1",
      "0abf284f3b56455f90f7e1705f9dbd49",
      "c118820e729247b3b74447ae2e084e77",
      "c3242776faa740debaf98fecc4b2ebd5",
      "bc377045b65d415f973e5dc75ac34b16"
     ]
    },
    "id": "E5_oIwgRH1Rt",
    "outputId": "0fbe2203-5568-4283-aa45-6ac8ba667fb2"
   },
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# FINE-TUNING GPT-2 SUR CNN/DAILYMAIL (5000 exemples)\n",
    "# ==============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINE-TUNING GPT-2 (124M) - COMME L'ARTICLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==============================================\n",
    "# 1. INSTALLATIONS\n",
    "# ==============================================\n",
    "\n",
    "!pip install transformers datasets accelerate rouge-score -q\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Nettoyage m√©moire\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques install√©es\")\n",
    "\n",
    "# ==============================================\n",
    "# 2. DATASET (5000 train, 1000 val, 1000 test)\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä CHARGEMENT DU DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# Split comme dans l'article\n",
    "train_dataset = dataset[\"train\"].select(range(5000))      # 5000 training\n",
    "val_dataset = dataset[\"validation\"].select(range(1000))   # 1000 validation\n",
    "test_dataset = dataset[\"test\"].select(range(1000))        # 1000 test\n",
    "\n",
    "print(f\"‚úÖ Dataset pr√™t:\")\n",
    "print(f\"  Training:   {len(train_dataset)} exemples\")\n",
    "print(f\"  Validation: {len(val_dataset)} exemples\")\n",
    "print(f\"  Test:       {len(test_dataset)} exemples\")\n",
    "\n",
    "# ==============================================\n",
    "# 3. TOKENISATION GPT-2\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî§ TOKENISATION GPT-2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Important pour GPT-2\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Format pour GPT-2: [article] [separator] [summary]\"\"\"\n",
    "    texts = []\n",
    "\n",
    "    for article, highlight in zip(examples[\"article\"], examples[\"highlights\"]):\n",
    "        # Format: article + s√©parateur + r√©sum√©\n",
    "        text = f\"ARTICLE: {article}\\n\\nSUMMARY: {highlight}\"\n",
    "        texts.append(text)\n",
    "\n",
    "    # Tokeniser\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # Pour GPT-2, les labels sont les m√™mes que input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenisation en cours...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenisation training\"\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenisation validation\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenisation termin√©e\")\n",
    "\n",
    "# ==============================================\n",
    "# 4. MOD√àLE GPT-2 (124M param√®tres)\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß† CHARGEMENT DE GPT-2 (124M)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Configurer pour le padding\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úÖ GPT-2 charg√©\")\n",
    "print(f\"üìä Param√®tres: {total_params/1e6:.1f}M\")\n",
    "print(f\"üìä Device: {model.device}\")\n",
    "\n",
    "# ==============================================\n",
    "# 5. CONFIGURATION DU FINE-TUNING\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚öôÔ∏è  CONFIGURATION DU FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned-5000\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,  # Comme dans l'article\n",
    "    per_device_train_batch_size=4,  # GPT-2 est plus l√©ger\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Batch effectif = 16\n",
    "    learning_rate=1e-5,  # Faible comme dans l'article\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs-gpt2\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration d√©finie:\")\n",
    "print(f\"  ‚Ä¢ Epochs: 5 (comme l'article)\")\n",
    "print(f\"  ‚Ä¢ Batch size: 4\")\n",
    "print(f\"  ‚Ä¢ Learning rate: 1e-5\")\n",
    "\n",
    "# ==============================================\n",
    "# 6. FINE-TUNING GPT-2\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî• D√âBUT DU FINE-TUNING GPT-2\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚ö†Ô∏è  Cette √©tape prend 1-2 heures\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    print(f\"\\n‚úÖ FINE-TUNING R√âUSSI !\")\n",
    "    print(f\"‚è±Ô∏è  Temps: {train_result.metrics['train_runtime']/60:.1f} min\")\n",
    "    print(f\"üìâ Training loss: {train_result.metrics['train_loss']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERREUR: {e}\")\n",
    "    print(\"\\nüîÑ Tentative avec batch_size=2...\")\n",
    "\n",
    "    # R√©essayer avec batch size plus petit\n",
    "    training_args.per_device_train_batch_size = 2\n",
    "    training_args.per_device_eval_batch_size = 2\n",
    "    training_args.gradient_accumulation_steps = 8\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    print(f\"\\n‚úÖ FINE-TUNING R√âUSSI avec batch_size=2\")\n",
    "\n",
    "# ==============================================\n",
    "# 7. SAUVEGARDE DU MOD√àLE FINE-TUN√â\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ SAUVEGARDE DU MOD√àLE GPT-2 FINE-TUN√â\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_save_path = \"./gpt2_finetuned_5000\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le GPT-2 fine-tun√© sauvegard√© dans: {model_save_path}\")\n",
    "\n",
    "# ==============================================\n",
    "# 8. √âVALUATION SUR TEST SET (1000 exemples)\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä √âVALUATION ROUGE SUR TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "!pip install rouge-score -q\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "\n",
    "# Fonction de g√©n√©ration pour GPT-2\n",
    "def generate_summary_gpt2(text):\n",
    "    \"\"\"G√©n√®re un r√©sum√© avec GPT-2 fine-tun√© - CORRIG√â\"\"\"\n",
    "    prompt = f\"ARTICLE: {text[:800]}\\n\\nSUMMARY:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],  # ‚≠ê AJOUTER\n",
    "            max_new_tokens=100,  # ‚≠ê CORRECTION ICI (au lieu de max_length)\n",
    "            min_new_tokens=30,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            num_beams=2,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"SUMMARY:\" in full_output:\n",
    "        return full_output.split(\"SUMMARY:\")[-1].strip()\n",
    "    return full_output\n",
    "\n",
    "# √âvaluation sur 1000 exemples\n",
    "print(f\"√âvaluation sur 1000 exemples du test set...\")\n",
    "\n",
    "gpt2_rouge1 = []\n",
    "gpt2_rouge2 = []\n",
    "gpt2_rougeL = []\n",
    "gpt2_rougeLsum = []\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(1000):\n",
    "    article = test_dataset[i][\"article\"]\n",
    "    reference = test_dataset[i][\"highlights\"]\n",
    "\n",
    "    generated = generate_summary_gpt2(article)\n",
    "    scores = scorer.score(reference, generated)\n",
    "\n",
    "    gpt2_rouge1.append(scores['rouge1'].fmeasure)\n",
    "    gpt2_rouge2.append(scores['rouge2'].fmeasure)\n",
    "    gpt2_rougeL.append(scores['rougeL'].fmeasure)\n",
    "    gpt2_rougeLsum.append(scores['rougeLsum'].fmeasure)\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        progress = (i + 1) / 1000 * 100\n",
    "        current_rouge1 = np.mean(gpt2_rouge1) * 100\n",
    "        print(f\"  {i+1}/1000 ({progress:.0f}%) - ROUGE-1: {current_rouge1:.1f}%\")\n",
    "\n",
    "eval_time = time.time() - start_time\n",
    "\n",
    "# ==============================================\n",
    "# 9. R√âSULTATS ROUGE (comme l'article)\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà R√âSULTATS ROUGE - GPT-2 FINE-TUN√â\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gpt2_r1 = np.mean(gpt2_rouge1) * 100\n",
    "gpt2_r2 = np.mean(gpt2_rouge2) * 100\n",
    "gpt2_rL = np.mean(gpt2_rougeL) * 100\n",
    "gpt2_rLsum = np.mean(gpt2_rougeLsum) * 100\n",
    "\n",
    "print(f\"\\nüéØ TES R√âSULTATS GPT-2 (1000 exemples):\")\n",
    "print(f\"  ROUGE-1:    {gpt2_r1:.2f}%\")\n",
    "print(f\"  ROUGE-2:    {gpt2_r2:.2f}%\")\n",
    "print(f\"  ROUGE-L:    {gpt2_rL:.2f}%\")\n",
    "print(f\"  ROUGE-Lsum: {gpt2_rLsum:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìä STATISTIQUES:\")\n",
    "print(f\"  √âcart-type ROUGE-1: {np.std(gpt2_rouge1)*100:.2f}%\")\n",
    "print(f\"  Temps d'√©valuation: {eval_time/60:.1f} min\")\n",
    "\n",
    "# ==============================================\n",
    "# 10. COMPARAISON AVEC L'ARTICLE\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä COMPARAISON AVEC L'ARTICLE (Table 3)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'Mod√®le':<25} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10} {'ROUGE-Lsum':<10}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Article GPT-2':<25} {24.83:<10.2f} {16.92:<10.2f} {22.14:<10.2f} {21.07:<10.2f}\")\n",
    "print(f\"{'Ton GPT-2 (5000 ex)':<25} {gpt2_r1:<10.2f} {gpt2_r2:<10.2f} {gpt2_rL:<10.2f} {gpt2_rLsum:<10.2f}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "difference_rouge1 = gpt2_r1 - 24.83\n",
    "print(f\"\\nüìà Diff√©rence ROUGE-1: {difference_rouge1:+.2f}%\")\n",
    "\n",
    "if difference_rouge1 > 0:\n",
    "    print(\"‚úÖ Ton mod√®le performe MIEUX que l'article !\")\n",
    "elif difference_rouge1 > -5:\n",
    "    print(\"üëç Performance proche de l'article (normal avec moins de donn√©es)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Performance inf√©rieure (normal: 5000 vs 287K exemples dans l'article)\")\n",
    "\n",
    "# ==============================================\n",
    "# 11. SAUVEGARDE DES R√âSULTATS\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ SAUVEGARDE DES R√âSULTATS GPT-2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Cr√©er dossier r√©sultats\n",
    "results_dir = \"./gpt2_finetuned_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Sauvegarder les r√©sultats\n",
    "results = {\n",
    "    \"model\": \"GPT-2 (124M) fine-tuned\",\n",
    "    \"training\": {\n",
    "        \"examples\": 5000,\n",
    "        \"validation\": 1000,\n",
    "        \"epochs\": 5,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 4\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"test_examples\": 1000,\n",
    "        \"rouge1\": float(gpt2_r1),\n",
    "        \"rouge2\": float(gpt2_r2),\n",
    "        \"rougeL\": float(gpt2_rL),\n",
    "        \"rougeLsum\": float(gpt2_rLsum),\n",
    "        \"std_rouge1\": float(np.std(gpt2_rouge1) * 100)\n",
    "    },\n",
    "    \"comparison_with_article\": {\n",
    "        \"article_rouge1\": 24.83,\n",
    "        \"article_rouge2\": 16.92,\n",
    "        \"article_rougeL\": 22.14,\n",
    "        \"article_rougeLsum\": 21.07,\n",
    "        \"difference_rouge1\": float(difference_rouge1)\n",
    "    },\n",
    "    \"date\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(results_dir, \"results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ R√©sultats sauvegard√©s dans: {results_dir}/results.json\")\n",
    "\n",
    "# ==============================================\n",
    "# 12. COMPARAISON GPT-2 vs BART\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä COMPARAISON GPT-2 vs BART\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Charger r√©sultats BART si disponibles\n",
    "bart_results_path = \"./bart_finetuned_results/results.json\"\n",
    "if os.path.exists(bart_results_path):\n",
    "    with open(bart_results_path, 'r') as f:\n",
    "        bart_results = json.load(f)\n",
    "\n",
    "    bart_rouge1 = bart_results[\"evaluation\"][\"rouge1\"]\n",
    "\n",
    "    print(f\"\\nüéØ COMPARAISON DES PERFORMANCES:\")\n",
    "    print(f\"  ‚Ä¢ GPT-2 ROUGE-1: {gpt2_r1:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ BART ROUGE-1:  {bart_rouge1:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Diff√©rence:    {bart_rouge1 - gpt2_r1:+.2f}%\")\n",
    "\n",
    "    if bart_rouge1 > gpt2_r1:\n",
    "        print(f\"\\n‚úÖ BART est sup√©rieur √† GPT-2 (confirm√©)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  R√©sultat inattendu\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  R√©sultats BART non trouv√©s pour comparaison\")\n",
    "\n",
    "# ==============================================\n",
    "# 13. T√âL√âCHARGEMENT\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ PR√âPARATION DU T√âL√âCHARGEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Cr√©er ZIP avec mod√®le + r√©sultats\n",
    "final_dir = \"./gpt2_project_final\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "# Copier mod√®le\n",
    "shutil.copytree(model_save_path, os.path.join(final_dir, \"model\"), dirs_exist_ok=True)\n",
    "# Copier r√©sultats\n",
    "shutil.copy(os.path.join(results_dir, \"results.json\"), os.path.join(final_dir, \"results.json\"))\n",
    "\n",
    "# Cr√©er ZIP\n",
    "zip_name = \"gpt2_finetuned_project\"\n",
    "shutil.make_archive(zip_name, 'zip', final_dir)\n",
    "\n",
    "# T√©l√©charger\n",
    "from google.colab import files\n",
    "files.download(f\"{zip_name}.zip\")\n",
    "\n",
    "print(f\"\\n‚úÖ PROJET GPT-2 TERMIN√â !\")\n",
    "print(f\"üì¶ Fichier: {zip_name}.zip\")\n",
    "print(f\"üìä ROUGE-1: {gpt2_r1:.2f}%\")\n",
    "print(f\"üìà Comparaison article: {difference_rouge1:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KaQiGJ9H3fc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
