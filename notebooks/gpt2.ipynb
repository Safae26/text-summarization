{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6086efb318fd40ec8268bfc76870cb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3cfa1074ec04645b04589877f033c06",
              "IPY_MODEL_1e9364be982040ec99adc9aeea618b76",
              "IPY_MODEL_465dc95b14ef468fa5d1a725237918ef"
            ],
            "layout": "IPY_MODEL_2bf1b140b3f24fe58a32f44afa1be98b"
          }
        },
        "a3cfa1074ec04645b04589877f033c06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab74299e791f4bf7b334f968476f1ea7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1e17b23ee018422b85f77ec2940e4eae",
            "value": "Tokenisation‚Äátraining:‚Äá100%"
          }
        },
        "1e9364be982040ec99adc9aeea618b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47d2431fafa24761b82fd474c77cee50",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c26102c34ed4194a9b8892d0c678ea2",
            "value": 5000
          }
        },
        "465dc95b14ef468fa5d1a725237918ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccf858fa5811408c8cef8630c07f4962",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1f3f78c41b894d19bdf26720885ab926",
            "value": "‚Äá5000/5000‚Äá[00:35&lt;00:00,‚Äá109.58‚Äáexamples/s]"
          }
        },
        "2bf1b140b3f24fe58a32f44afa1be98b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab74299e791f4bf7b334f968476f1ea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e17b23ee018422b85f77ec2940e4eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47d2431fafa24761b82fd474c77cee50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c26102c34ed4194a9b8892d0c678ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccf858fa5811408c8cef8630c07f4962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f3f78c41b894d19bdf26720885ab926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1399b6599673411aaaddfbd0fe09ef10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f76f95597af840b79f0c629620207883",
              "IPY_MODEL_0094a884c0ea4d87b12765133b2612b4",
              "IPY_MODEL_9704440d598346a99d9f5ff9b315d1a8"
            ],
            "layout": "IPY_MODEL_e5414688df1541a7b71558898fbc36c4"
          }
        },
        "f76f95597af840b79f0c629620207883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abc91cb5433849f98097c1e5c0837c92",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f79059326b5544e0825abd353ae58cb1",
            "value": "Tokenisation‚Äávalidation:‚Äá100%"
          }
        },
        "0094a884c0ea4d87b12765133b2612b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0abf284f3b56455f90f7e1705f9dbd49",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c118820e729247b3b74447ae2e084e77",
            "value": 1000
          }
        },
        "9704440d598346a99d9f5ff9b315d1a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3242776faa740debaf98fecc4b2ebd5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bc377045b65d415f973e5dc75ac34b16",
            "value": "‚Äá1000/1000‚Äá[00:06&lt;00:00,‚Äá179.90‚Äáexamples/s]"
          }
        },
        "e5414688df1541a7b71558898fbc36c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc91cb5433849f98097c1e5c0837c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f79059326b5544e0825abd353ae58cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0abf284f3b56455f90f7e1705f9dbd49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c118820e729247b3b74447ae2e084e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3242776faa740debaf98fecc4b2ebd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc377045b65d415f973e5dc75ac34b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Safae26/text-summarization/blob/main/notebooks/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6086efb318fd40ec8268bfc76870cb8b",
            "a3cfa1074ec04645b04589877f033c06",
            "1e9364be982040ec99adc9aeea618b76",
            "465dc95b14ef468fa5d1a725237918ef",
            "2bf1b140b3f24fe58a32f44afa1be98b",
            "ab74299e791f4bf7b334f968476f1ea7",
            "1e17b23ee018422b85f77ec2940e4eae",
            "47d2431fafa24761b82fd474c77cee50",
            "1c26102c34ed4194a9b8892d0c678ea2",
            "ccf858fa5811408c8cef8630c07f4962",
            "1f3f78c41b894d19bdf26720885ab926",
            "1399b6599673411aaaddfbd0fe09ef10",
            "f76f95597af840b79f0c629620207883",
            "0094a884c0ea4d87b12765133b2612b4",
            "9704440d598346a99d9f5ff9b315d1a8",
            "e5414688df1541a7b71558898fbc36c4",
            "abc91cb5433849f98097c1e5c0837c92",
            "f79059326b5544e0825abd353ae58cb1",
            "0abf284f3b56455f90f7e1705f9dbd49",
            "c118820e729247b3b74447ae2e084e77",
            "c3242776faa740debaf98fecc4b2ebd5",
            "bc377045b65d415f973e5dc75ac34b16"
          ]
        },
        "id": "E5_oIwgRH1Rt",
        "outputId": "0fbe2203-5568-4283-aa45-6ac8ba667fb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FINE-TUNING GPT-2 (124M) - COMME L'ARTICLE\n",
            "============================================================\n",
            "‚úÖ Biblioth√®ques install√©es\n",
            "\n",
            "============================================================\n",
            "üìä CHARGEMENT DU DATASET\n",
            "============================================================\n",
            "‚úÖ Dataset pr√™t:\n",
            "  Training:   5000 exemples\n",
            "  Validation: 1000 exemples\n",
            "  Test:       1000 exemples\n",
            "\n",
            "============================================================\n",
            "üî§ TOKENISATION GPT-2\n",
            "============================================================\n",
            "Tokenisation en cours...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenisation training:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6086efb318fd40ec8268bfc76870cb8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenisation validation:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1399b6599673411aaaddfbd0fe09ef10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tokenisation termin√©e\n",
            "\n",
            "============================================================\n",
            "üß† CHARGEMENT DE GPT-2 (124M)\n",
            "============================================================\n",
            "‚úÖ GPT-2 charg√©\n",
            "üìä Param√®tres: 124.4M\n",
            "üìä Device: cpu\n",
            "\n",
            "============================================================\n",
            "‚öôÔ∏è  CONFIGURATION DU FINE-TUNING\n",
            "============================================================\n",
            "‚úÖ Configuration d√©finie:\n",
            "  ‚Ä¢ Epochs: 5 (comme l'article)\n",
            "  ‚Ä¢ Batch size: 4\n",
            "  ‚Ä¢ Learning rate: 1e-5\n",
            "\n",
            "============================================================\n",
            "üî• D√âBUT DU FINE-TUNING GPT-2\n",
            "============================================================\n",
            "‚ö†Ô∏è  Cette √©tape prend 1-2 heures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1122273437.py:167: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1565' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1565/1565 31:32, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.786400</td>\n",
              "      <td>2.756299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.753500</td>\n",
              "      <td>2.757755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.736100</td>\n",
              "      <td>2.758270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ FINE-TUNING R√âUSSI !\n",
            "‚è±Ô∏è  Temps: 31.6 min\n",
            "üìâ Training loss: 2.788\n",
            "\n",
            "============================================================\n",
            "üíæ SAUVEGARDE DU MOD√àLE GPT-2 FINE-TUN√â\n",
            "============================================================\n",
            "‚úÖ Mod√®le GPT-2 fine-tun√© sauvegard√© dans: ./gpt2_finetuned_5000\n",
            "\n",
            "============================================================\n",
            "üìä √âVALUATION ROUGE SUR TEST SET\n",
            "============================================================\n",
            "√âvaluation sur 1000 exemples du test set...\n",
            "  100/1000 (10%) - ROUGE-1: 23.3%\n",
            "  200/1000 (20%) - ROUGE-1: 23.2%\n",
            "  300/1000 (30%) - ROUGE-1: 23.1%\n",
            "  400/1000 (40%) - ROUGE-1: 23.4%\n",
            "  500/1000 (50%) - ROUGE-1: 23.1%\n",
            "  600/1000 (60%) - ROUGE-1: 23.0%\n",
            "  700/1000 (70%) - ROUGE-1: 23.2%\n",
            "  800/1000 (80%) - ROUGE-1: 23.1%\n",
            "  900/1000 (90%) - ROUGE-1: 23.0%\n",
            "  1000/1000 (100%) - ROUGE-1: 22.9%\n",
            "\n",
            "============================================================\n",
            "üìà R√âSULTATS ROUGE - GPT-2 FINE-TUN√â\n",
            "============================================================\n",
            "\n",
            "üéØ TES R√âSULTATS GPT-2 (1000 exemples):\n",
            "  ROUGE-1:    22.92%\n",
            "  ROUGE-2:    8.01%\n",
            "  ROUGE-L:    17.65%\n",
            "  ROUGE-Lsum: 19.84%\n",
            "\n",
            "üìä STATISTIQUES:\n",
            "  √âcart-type ROUGE-1: 11.87%\n",
            "  Temps d'√©valuation: 19.7 min\n",
            "\n",
            "============================================================\n",
            "üìä COMPARAISON AVEC L'ARTICLE (Table 3)\n",
            "============================================================\n",
            "\n",
            "Mod√®le                    ROUGE-1    ROUGE-2    ROUGE-L    ROUGE-Lsum\n",
            "-----------------------------------------------------------------\n",
            "Article GPT-2             24.83      16.92      22.14      21.07     \n",
            "Ton GPT-2 (5000 ex)       22.92      8.01       17.65      19.84     \n",
            "-----------------------------------------------------------------\n",
            "\n",
            "üìà Diff√©rence ROUGE-1: -1.91%\n",
            "üëç Performance proche de l'article (normal avec moins de donn√©es)\n",
            "\n",
            "============================================================\n",
            "üíæ SAUVEGARDE DES R√âSULTATS GPT-2\n",
            "============================================================\n",
            "‚úÖ R√©sultats sauvegard√©s dans: ./gpt2_finetuned_results/results.json\n",
            "\n",
            "============================================================\n",
            "üìä COMPARAISON GPT-2 vs BART\n",
            "============================================================\n",
            "‚ö†Ô∏è  R√©sultats BART non trouv√©s pour comparaison\n",
            "\n",
            "============================================================\n",
            "üì¶ PR√âPARATION DU T√âL√âCHARGEMENT\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c9a03257-d796-4305-8d8a-ef1f19f5b6b3\", \"gpt2_finetuned_project.zip\", 463241429)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ PROJET GPT-2 TERMIN√â !\n",
            "üì¶ Fichier: gpt2_finetuned_project.zip\n",
            "üìä ROUGE-1: 22.92%\n",
            "üìà Comparaison article: -1.91%\n"
          ]
        }
      ],
      "source": [
        "# ==============================================\n",
        "# FINE-TUNING GPT-2 SUR CNN/DAILYMAIL (5000 exemples)\n",
        "# ==============================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FINE-TUNING GPT-2 (124M) - COMME L'ARTICLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ==============================================\n",
        "# 1. INSTALLATIONS\n",
        "# ==============================================\n",
        "\n",
        "!pip install transformers datasets accelerate rouge-score -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# Nettoyage m√©moire\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "\n",
        "print(\"‚úÖ Biblioth√®ques install√©es\")\n",
        "\n",
        "# ==============================================\n",
        "# 2. DATASET (5000 train, 1000 val, 1000 test)\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä CHARGEMENT DU DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "# Split comme dans l'article\n",
        "train_dataset = dataset[\"train\"].select(range(5000))      # 5000 training\n",
        "val_dataset = dataset[\"validation\"].select(range(1000))   # 1000 validation\n",
        "test_dataset = dataset[\"test\"].select(range(1000))        # 1000 test\n",
        "\n",
        "print(f\"‚úÖ Dataset pr√™t:\")\n",
        "print(f\"  Training:   {len(train_dataset)} exemples\")\n",
        "print(f\"  Validation: {len(val_dataset)} exemples\")\n",
        "print(f\"  Test:       {len(test_dataset)} exemples\")\n",
        "\n",
        "# ==============================================\n",
        "# 3. TOKENISATION GPT-2\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî§ TOKENISATION GPT-2\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Important pour GPT-2\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Format pour GPT-2: [article] [separator] [summary]\"\"\"\n",
        "    texts = []\n",
        "\n",
        "    for article, highlight in zip(examples[\"article\"], examples[\"highlights\"]):\n",
        "        # Format: article + s√©parateur + r√©sum√©\n",
        "        text = f\"ARTICLE: {article}\\n\\nSUMMARY: {highlight}\"\n",
        "        texts.append(text)\n",
        "\n",
        "    # Tokeniser\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    # Pour GPT-2, les labels sont les m√™mes que input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenisation en cours...\")\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=8,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenisation training\"\n",
        ")\n",
        "\n",
        "tokenized_val = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=8,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Tokenisation validation\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Tokenisation termin√©e\")\n",
        "\n",
        "# ==============================================\n",
        "# 4. MOD√àLE GPT-2 (124M param√®tres)\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß† CHARGEMENT DE GPT-2 (124M)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Configurer pour le padding\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ GPT-2 charg√©\")\n",
        "print(f\"üìä Param√®tres: {total_params/1e6:.1f}M\")\n",
        "print(f\"üìä Device: {model.device}\")\n",
        "\n",
        "# ==============================================\n",
        "# 5. CONFIGURATION DU FINE-TUNING\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚öôÔ∏è  CONFIGURATION DU FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned-5000\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,  # Comme dans l'article\n",
        "    per_device_train_batch_size=4,  # GPT-2 est plus l√©ger\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Batch effectif = 16\n",
        "    learning_rate=1e-5,  # Faible comme dans l'article\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs-gpt2\",\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    dataloader_pin_memory=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Configuration d√©finie:\")\n",
        "print(f\"  ‚Ä¢ Epochs: 5 (comme l'article)\")\n",
        "print(f\"  ‚Ä¢ Batch size: 4\")\n",
        "print(f\"  ‚Ä¢ Learning rate: 1e-5\")\n",
        "\n",
        "# ==============================================\n",
        "# 6. FINE-TUNING GPT-2\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî• D√âBUT DU FINE-TUNING GPT-2\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚ö†Ô∏è  Cette √©tape prend 1-2 heures\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    print(f\"\\n‚úÖ FINE-TUNING R√âUSSI !\")\n",
        "    print(f\"‚è±Ô∏è  Temps: {train_result.metrics['train_runtime']/60:.1f} min\")\n",
        "    print(f\"üìâ Training loss: {train_result.metrics['train_loss']:.3f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ERREUR: {e}\")\n",
        "    print(\"\\nüîÑ Tentative avec batch_size=2...\")\n",
        "\n",
        "    # R√©essayer avec batch size plus petit\n",
        "    training_args.per_device_train_batch_size = 2\n",
        "    training_args.per_device_eval_batch_size = 2\n",
        "    training_args.gradient_accumulation_steps = 8\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_val,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    train_result = trainer.train()\n",
        "    print(f\"\\n‚úÖ FINE-TUNING R√âUSSI avec batch_size=2\")\n",
        "\n",
        "# ==============================================\n",
        "# 7. SAUVEGARDE DU MOD√àLE FINE-TUN√â\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üíæ SAUVEGARDE DU MOD√àLE GPT-2 FINE-TUN√â\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_save_path = \"./gpt2_finetuned_5000\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"‚úÖ Mod√®le GPT-2 fine-tun√© sauvegard√© dans: {model_save_path}\")\n",
        "\n",
        "# ==============================================\n",
        "# 8. √âVALUATION SUR TEST SET (1000 exemples)\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä √âVALUATION ROUGE SUR TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!pip install rouge-score -q\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
        "\n",
        "# Fonction de g√©n√©ration pour GPT-2\n",
        "def generate_summary_gpt2(text):\n",
        "    \"\"\"G√©n√®re un r√©sum√© avec GPT-2 fine-tun√© - CORRIG√â\"\"\"\n",
        "    prompt = f\"ARTICLE: {text[:800]}\\n\\nSUMMARY:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],  # ‚≠ê AJOUTER\n",
        "            max_new_tokens=100,  # ‚≠ê CORRECTION ICI (au lieu de max_length)\n",
        "            min_new_tokens=30,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            num_beams=2,\n",
        "            early_stopping=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    full_output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    if \"SUMMARY:\" in full_output:\n",
        "        return full_output.split(\"SUMMARY:\")[-1].strip()\n",
        "    return full_output\n",
        "\n",
        "# √âvaluation sur 1000 exemples\n",
        "print(f\"√âvaluation sur 1000 exemples du test set...\")\n",
        "\n",
        "gpt2_rouge1 = []\n",
        "gpt2_rouge2 = []\n",
        "gpt2_rougeL = []\n",
        "gpt2_rougeLsum = []\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(1000):\n",
        "    article = test_dataset[i][\"article\"]\n",
        "    reference = test_dataset[i][\"highlights\"]\n",
        "\n",
        "    generated = generate_summary_gpt2(article)\n",
        "    scores = scorer.score(reference, generated)\n",
        "\n",
        "    gpt2_rouge1.append(scores['rouge1'].fmeasure)\n",
        "    gpt2_rouge2.append(scores['rouge2'].fmeasure)\n",
        "    gpt2_rougeL.append(scores['rougeL'].fmeasure)\n",
        "    gpt2_rougeLsum.append(scores['rougeLsum'].fmeasure)\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        progress = (i + 1) / 1000 * 100\n",
        "        current_rouge1 = np.mean(gpt2_rouge1) * 100\n",
        "        print(f\"  {i+1}/1000 ({progress:.0f}%) - ROUGE-1: {current_rouge1:.1f}%\")\n",
        "\n",
        "eval_time = time.time() - start_time\n",
        "\n",
        "# ==============================================\n",
        "# 9. R√âSULTATS ROUGE (comme l'article)\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà R√âSULTATS ROUGE - GPT-2 FINE-TUN√â\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "gpt2_r1 = np.mean(gpt2_rouge1) * 100\n",
        "gpt2_r2 = np.mean(gpt2_rouge2) * 100\n",
        "gpt2_rL = np.mean(gpt2_rougeL) * 100\n",
        "gpt2_rLsum = np.mean(gpt2_rougeLsum) * 100\n",
        "\n",
        "print(f\"\\nüéØ TES R√âSULTATS GPT-2 (1000 exemples):\")\n",
        "print(f\"  ROUGE-1:    {gpt2_r1:.2f}%\")\n",
        "print(f\"  ROUGE-2:    {gpt2_r2:.2f}%\")\n",
        "print(f\"  ROUGE-L:    {gpt2_rL:.2f}%\")\n",
        "print(f\"  ROUGE-Lsum: {gpt2_rLsum:.2f}%\")\n",
        "\n",
        "print(f\"\\nüìä STATISTIQUES:\")\n",
        "print(f\"  √âcart-type ROUGE-1: {np.std(gpt2_rouge1)*100:.2f}%\")\n",
        "print(f\"  Temps d'√©valuation: {eval_time/60:.1f} min\")\n",
        "\n",
        "# ==============================================\n",
        "# 10. COMPARAISON AVEC L'ARTICLE\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä COMPARAISON AVEC L'ARTICLE (Table 3)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n{'Mod√®le':<25} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10} {'ROUGE-Lsum':<10}\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'Article GPT-2':<25} {24.83:<10.2f} {16.92:<10.2f} {22.14:<10.2f} {21.07:<10.2f}\")\n",
        "print(f\"{'Ton GPT-2 (5000 ex)':<25} {gpt2_r1:<10.2f} {gpt2_r2:<10.2f} {gpt2_rL:<10.2f} {gpt2_rLsum:<10.2f}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "difference_rouge1 = gpt2_r1 - 24.83\n",
        "print(f\"\\nüìà Diff√©rence ROUGE-1: {difference_rouge1:+.2f}%\")\n",
        "\n",
        "if difference_rouge1 > 0:\n",
        "    print(\"‚úÖ Ton mod√®le performe MIEUX que l'article !\")\n",
        "elif difference_rouge1 > -5:\n",
        "    print(\"üëç Performance proche de l'article (normal avec moins de donn√©es)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Performance inf√©rieure (normal: 5000 vs 287K exemples dans l'article)\")\n",
        "\n",
        "# ==============================================\n",
        "# 11. SAUVEGARDE DES R√âSULTATS\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üíæ SAUVEGARDE DES R√âSULTATS GPT-2\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Cr√©er dossier r√©sultats\n",
        "results_dir = \"./gpt2_finetuned_results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Sauvegarder les r√©sultats\n",
        "results = {\n",
        "    \"model\": \"GPT-2 (124M) fine-tuned\",\n",
        "    \"training\": {\n",
        "        \"examples\": 5000,\n",
        "        \"validation\": 1000,\n",
        "        \"epochs\": 5,\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"batch_size\": 4\n",
        "    },\n",
        "    \"evaluation\": {\n",
        "        \"test_examples\": 1000,\n",
        "        \"rouge1\": float(gpt2_r1),\n",
        "        \"rouge2\": float(gpt2_r2),\n",
        "        \"rougeL\": float(gpt2_rL),\n",
        "        \"rougeLsum\": float(gpt2_rLsum),\n",
        "        \"std_rouge1\": float(np.std(gpt2_rouge1) * 100)\n",
        "    },\n",
        "    \"comparison_with_article\": {\n",
        "        \"article_rouge1\": 24.83,\n",
        "        \"article_rouge2\": 16.92,\n",
        "        \"article_rougeL\": 22.14,\n",
        "        \"article_rougeLsum\": 21.07,\n",
        "        \"difference_rouge1\": float(difference_rouge1)\n",
        "    },\n",
        "    \"date\": datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open(os.path.join(results_dir, \"results.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ R√©sultats sauvegard√©s dans: {results_dir}/results.json\")\n",
        "\n",
        "# ==============================================\n",
        "# 12. COMPARAISON GPT-2 vs BART\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä COMPARAISON GPT-2 vs BART\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Charger r√©sultats BART si disponibles\n",
        "bart_results_path = \"./bart_finetuned_results/results.json\"\n",
        "if os.path.exists(bart_results_path):\n",
        "    with open(bart_results_path, 'r') as f:\n",
        "        bart_results = json.load(f)\n",
        "\n",
        "    bart_rouge1 = bart_results[\"evaluation\"][\"rouge1\"]\n",
        "\n",
        "    print(f\"\\nüéØ COMPARAISON DES PERFORMANCES:\")\n",
        "    print(f\"  ‚Ä¢ GPT-2 ROUGE-1: {gpt2_r1:.2f}%\")\n",
        "    print(f\"  ‚Ä¢ BART ROUGE-1:  {bart_rouge1:.2f}%\")\n",
        "    print(f\"  ‚Ä¢ Diff√©rence:    {bart_rouge1 - gpt2_r1:+.2f}%\")\n",
        "\n",
        "    if bart_rouge1 > gpt2_r1:\n",
        "        print(f\"\\n‚úÖ BART est sup√©rieur √† GPT-2 (confirm√©)\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  R√©sultat inattendu\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  R√©sultats BART non trouv√©s pour comparaison\")\n",
        "\n",
        "# ==============================================\n",
        "# 13. T√âL√âCHARGEMENT\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üì¶ PR√âPARATION DU T√âL√âCHARGEMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Cr√©er ZIP avec mod√®le + r√©sultats\n",
        "final_dir = \"./gpt2_project_final\"\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "# Copier mod√®le\n",
        "shutil.copytree(model_save_path, os.path.join(final_dir, \"model\"), dirs_exist_ok=True)\n",
        "# Copier r√©sultats\n",
        "shutil.copy(os.path.join(results_dir, \"results.json\"), os.path.join(final_dir, \"results.json\"))\n",
        "\n",
        "# Cr√©er ZIP\n",
        "zip_name = \"gpt2_finetuned_project\"\n",
        "shutil.make_archive(zip_name, 'zip', final_dir)\n",
        "\n",
        "# T√©l√©charger\n",
        "from google.colab import files\n",
        "files.download(f\"{zip_name}.zip\")\n",
        "\n",
        "print(f\"\\n‚úÖ PROJET GPT-2 TERMIN√â !\")\n",
        "print(f\"üì¶ Fichier: {zip_name}.zip\")\n",
        "print(f\"üìä ROUGE-1: {gpt2_r1:.2f}%\")\n",
        "print(f\"üìà Comparaison article: {difference_rouge1:+.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KaQiGJ9H3fc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}